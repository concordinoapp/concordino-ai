{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sort intro uses Keras for :\n",
    "1. Uses Keras to load predefined dataset\n",
    "2. Create an automatic neureal network machine learning model that classifies images\n",
    "3. Train this neural network\n",
    "4. Evaluate the accuracy of the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure TensorFlow\n",
    "Start by importing TensorFlow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.11.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "\n",
    "Load the MNIST dataset. Convert the whole number examples in flating point number.\n",
    "The pixel values of the images range from 0 trough 255. Scale these values to a range of 0 to 1 by dividing the values by 255.0. This also convert from int to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy\n",
    "\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # These are matrices the max value is 255 so we divide by 255 to get values float values between 0 and 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a machine learning model\n",
    "\n",
    "Create an automatic learning model\n",
    "Create a model tf.keras.Sequential by stacking layers \n",
    "\n",
    "1. I Keras, there is 2 ways of building models : \n",
    "\n",
    "    - **Sequential** model : Allow to build model layer by layer. Does not allow to create models that share layers or have multiple inputs or outputs.\n",
    "    - **Functional** model : Allow to create fare more complexe models. Layers are not only connected to the previous and the next one but can be connected to any other one. \n",
    "    \n",
    "        This allow to create networks such as siamese or residual networks\n",
    "    - So Sequential is useful for stacking layers where each layer has one input **tensor** and one output **tensor**\n",
    "    - A **tensor** is an array in Tensorflow. It is like a np.arrays in NumPy. Basically, they are used like matrices in the neural networks computations. they can be rank-0 (constant array), rank-1 (1-D array), rank-2 (2-D array), etc...\n",
    "    - Layers are function with a known mathematical structure that can be reused and have trainable variables. Most Tensorflow models are composed of layers.\n",
    "    - **Flatten layer** : collapses the spatial dimension of the input into the channel dimension. Ex : input = H by W by C by N by S array (sequence oif images), then the flattened output is (H\\*W\\*B) by N by S array. \n",
    "\n",
    "        In our example, the Dense layer is using a 128-d vector so the Flatten layer will automaticcaly transform the input to match the requirements of the Dense layer\n",
    "    - **Dense layer** : The Dense layer is a neural network that is **connected deeply**. This means that each neuron in the dense layer is receives input from all neurons of its previous layer. Most commonly used layer in the models. \n",
    "        \n",
    "        In the background, the dense layer performs a matric-vector multiplication. The values used in the matrix are actually parameters that can be trained and updated with the help of backpropagation. \n",
    "        \n",
    "            keras.layers.Dense(units, activation=None, use_bias=True, ...) \n",
    "\n",
    "                - units = represents the output size of the layer. It is the unit parameter itself that plays a major role in the size of the weight matrix along the basic vector \n",
    "\n",
    "                - activation = activation function = function that decides wether a neuron should be activated or not by calculating the weighted sum and further adding bias to it. \n",
    "\n",
    "                **Weight*** increase the steepness of activation function = (how fast the activation function will trigger). Whereas the **Bias** is used to delay the triggerinf of the activation function. \n",
    "                It may be critical for successful training.\n",
    "                \n",
    "                The purpose is to introduce a non-linearity into the output of a neuron. Makes the back-propagation possible. \n",
    "                There is a lot of different activation functions with differnets outcomes and roles.  \n",
    "\n",
    "                - use_bias = wether we should use bias or not, default is True. \n",
    "\n",
    "    - **Dropout layer** : randomly sets input units to 0 with a frequency of rate at each step during training. = Drop some inputs, which help prevent overfitting. \n",
    "\n",
    "        - overfitting = a model that models the training data too well = rely too much on the training data instead of learning how to find output by itself. \n",
    "        - underfitting = a model that can neither model the training data nor generalize to new data. \n",
    "        - **Ideally, we want a model at the sweet spot between overfitting and underfitting.**\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               100480    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- The model returns a vector of **logits** or **log-odds** : \n",
    "    - **logits** : The vector of raw (= non-normalized) predictions that the classification model generates. **KEEP IN MIND THAT THE PREDICTIONS HAVE TO BE NORMALIZED USING A NORMALIZATION FUNCTION (LIKE SOFTMAX ?) AFTERWARD**\n",
    "    - **log-odds**: The logarithm of the odds of some events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.19953391, -0.13593133,  0.56630874,  0.49914253,  0.08583802,\n",
       "         0.04749148,  0.37313107,  0.0812885 ,  0.30607626, -0.6289251 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model(x_train[:1]).numpy() # !! we need to pass an array to the model so we can NOT call it with x_train[0] since it will returns only the value, this is why we use x_train[:1]. [3, 4, 5][0] = 3 WHEREAS [3, 4, 5][:1] = [3]\n",
    "predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _tf.nn.softmax_ functon converts these logits to probabilities for each class. This is the normalization function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.10115221, 0.07232438, 0.14597003, 0.1364878 , 0.09028132,\n",
       "        0.08688489, 0.12032827, 0.08987152, 0.11252425, 0.04417537]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.softmax(predictions).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: It is possible to bake the tf.nn.softmax function into the activation function for the last layer of the network. While this can make the model output more directly interpretable, this approach is discouraged as it's impossible to provide an exact and numerically stable loss calculation for all models when using a softmax output.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a loss unfciton for training using **losses.SparseCategoricalCrossentropy**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function takes a vector of ground truth values (= real value from dataset, value we know to be true) and a vector of logits anrd returns a scalar loss for each example. \n",
    "This loss is equal to the negative log probability of the true class: The loss is zero if the model is sure of the correct class.\n",
    "This untrained model gives probabilities close to random (1/10 for each class), so the initial loss should be close to ```tf.math.log(1/10) ~= 2.3```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.19953391 -0.13593133  0.56630874  0.49914253  0.08583802  0.04749148\n",
      "   0.37313107  0.0812885   0.30607626 -0.6289251 ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.4431713"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(y_train[:1], predictions).numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you start training, configure and compile the model using Keras ``Model.compile``. Set the optimize class to ``\"adam\"``, set the the ``loss`` ``loss_fn`` function you defined earlier, and specify a metric to be evaluated for the model by settings the ``metrics`` parameters to ``accuracy``.\n",
    "\n",
    "Adam is an optimization algorithm that can be used instead of the classical stochastic gradient descent procedure to update network weights iterative based in training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=loss_fn,\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate your model\n",
    "\n",
    "Use the ``Model.fit`` method to adjust your model parameters and minimize the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3057 - accuracy: 0.9106\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1473 - accuracy: 0.9558\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1104 - accuracy: 0.9672\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0896 - accuracy: 0.9731\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0748 - accuracy: 0.9769\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7006276c10>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``Model.evaluate`` method checks the model's performance, usually on a validation set or test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - loss: 0.0861 - accuracy: 0.9746 - 903ms/epoch - 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08612670749425888, 0.9746000170707703]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image classifier is now trained to ~98% accuracy on this dataset.\n",
    "\n",
    "If you want your model to return a probability, you can wrap the trained model, and attach the softmax to it :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 10), dtype=float32, numpy=\n",
       "array([[1.9309323e-06, 1.2683474e-10, 5.4431612e-06, 5.1775336e-05,\n",
       "        5.2585852e-11, 8.6468347e-08, 3.0214440e-12, 9.9993265e-01,\n",
       "        5.5024614e-08, 8.0744230e-06],\n",
       "       [3.4143693e-07, 1.3269081e-04, 9.9964666e-01, 2.1180366e-04,\n",
       "        2.2969859e-15, 5.3869958e-06, 1.1500309e-06, 1.3457478e-13,\n",
       "        2.0551868e-06, 6.7652771e-12],\n",
       "       [2.0512924e-07, 9.9895740e-01, 7.4604654e-06, 1.9420379e-05,\n",
       "        4.7965521e-05, 6.2324416e-06, 3.0984390e-06, 8.0779148e-04,\n",
       "        1.4771392e-04, 2.6819459e-06],\n",
       "       [9.9995613e-01, 1.2489338e-11, 1.2760277e-05, 8.7295876e-10,\n",
       "        2.0110861e-08, 2.9480293e-07, 3.0191983e-05, 4.8356782e-07,\n",
       "        2.0667862e-10, 1.3822645e-07],\n",
       "       [1.2937604e-06, 6.8424879e-09, 6.3373140e-05, 2.8516661e-07,\n",
       "        9.8487502e-01, 1.2192855e-06, 5.9674181e-07, 1.3881865e-04,\n",
       "        3.3172171e-06, 1.4916180e-02]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probability_model = tf.keras.Sequential([\n",
    "    model,\n",
    "    tf.keras.layers.Softmax()\n",
    "])\n",
    "\n",
    "probability_model(x_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step\n",
      "q len = 1\n",
      "result = 7\n",
      "found result = 7\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "q len = 1\n",
      "result = 2\n",
      "found result = 2\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "q len = 1\n",
      "result = 1\n",
      "found result = 1\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "q len = 1\n",
      "result = 0\n",
      "found result = 0\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "q len = 1\n",
      "result = 4\n",
      "found result = 4\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "q len = 1\n",
      "result = 1\n",
      "found result = 1\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "q len = 1\n",
      "result = 4\n",
      "found result = 4\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "q len = 1\n",
      "result = 9\n",
      "found result = 9\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "q len = 1\n",
      "result = 5\n",
      "found result = 5\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "q len = 1\n",
      "result = 9\n",
      "found result = 9\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "q len = 1\n",
      "result = 0\n",
      "found result = 0\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "q len = 1\n",
      "result = 6\n",
      "found result = 6\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "q len = 1\n",
      "result = 9\n",
      "found result = 9\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "q len = 1\n",
      "result = 0\n",
      "found result = 0\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "q len = 1\n",
      "result = 1\n",
      "found result = 1\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "q len = 1\n",
      "result = 5\n",
      "found result = 5\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "q len = 1\n",
      "result = 9\n",
      "found result = 9\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "q len = 1\n",
      "result = 7\n",
      "found result = 7\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "q len = 1\n",
      "result = 3\n",
      "found result = 3\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "q len = 1\n",
      "result = 4\n",
      "found result = 4\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "q len = 1\n",
      "result = 9\n",
      "found result = 9\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "q len = 1\n",
      "result = 6\n",
      "found result = 6\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "q len = 1\n",
      "result = 6\n",
      "found result = 6\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "q len = 1\n",
      "result = 5\n",
      "found result = 5\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "q len = 1\n",
      "result = 4\n",
      "found result = 4\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "q len = 1\n",
      "result = 0\n",
      "found result = 0\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "q len = 1\n",
      "result = 7\n",
      "found result = 7\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "q len = 1\n",
      "result = 4\n",
      "found result = 4\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "q len = 1\n",
      "result = 0\n",
      "found result = 0\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "q len = 1\n",
      "result = 1\n",
      "found result = 1\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "q len = 1\n",
      "result = 3\n",
      "found result = 3\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "q len = 1\n",
      "result = 1\n",
      "found result = 1\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "q len = 1\n",
      "result = 3\n",
      "found result = 3\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "q len = 1\n",
      "result = 4\n",
      "found result = 4\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "q len = 1\n",
      "result = 7\n",
      "found result = 7\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "q len = 1\n",
      "result = 2\n",
      "found result = 2\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "q len = 1\n",
      "result = 7\n",
      "found result = 7\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "q len = 1\n",
      "result = 1\n",
      "found result = 1\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "q len = 1\n",
      "result = 2\n",
      "found result = 2\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "q len = 1\n",
      "result = 1\n",
      "found result = 1\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "q len = 1\n",
      "result = 1\n",
      "found result = 1\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "q len = 1\n",
      "result = 7\n",
      "found result = 7\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "q len = 1\n",
      "result = 4\n",
      "found result = 4\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "q len = 1\n",
      "result = 2\n",
      "found result = 2\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "q len = 1\n",
      "result = 3\n",
      "found result = 3\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "q len = 1\n",
      "result = 5\n",
      "found result = 5\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "q len = 1\n",
      "result = 1\n",
      "found result = 1\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "q len = 1\n",
      "result = 2\n",
      "found result = 2\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "q len = 1\n",
      "result = 4\n",
      "found result = 4\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "q len = 1\n",
      "result = 4\n",
      "found result = 4\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "q len = 1\n",
      "result = 6\n",
      "found result = 6\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "q len = 1\n",
      "result = 3\n",
      "found result = 3\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "q len = 1\n",
      "result = 5\n",
      "found result = 5\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "q len = 1\n",
      "result = 5\n",
      "found result = 5\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "q len = 1\n",
      "result = 6\n",
      "found result = 6\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "q len = 1\n",
      "result = 0\n",
      "found result = 0\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "q len = 1\n",
      "result = 4\n",
      "found result = 4\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "q len = 1\n",
      "result = 1\n",
      "found result = 1\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "q len = 1\n",
      "result = 9\n",
      "found result = 9\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "q len = 1\n",
      "result = 5\n",
      "found result = 5\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "q len = 1\n",
      "result = 7\n",
      "found result = 7\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "q len = 1\n",
      "result = 8\n",
      "found result = 8\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "q len = 1\n",
      "result = 9\n",
      "found result = 9\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "q len = 1\n",
      "result = 3\n",
      "found result = 3\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "q len = 1\n",
      "result = 7\n",
      "found result = 7\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "q len = 1\n",
      "result = 4\n",
      "found result = 4\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "q len = 1\n",
      "result = 6\n",
      "found result = 6\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "q len = 1\n",
      "result = 4\n",
      "found result = 4\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "q len = 1\n",
      "result = 3\n",
      "found result = 3\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "q len = 1\n",
      "result = 0\n",
      "found result = 0\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "q len = 1\n",
      "result = 7\n",
      "found result = 7\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "q len = 1\n",
      "result = 0\n",
      "found result = 0\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "q len = 1\n",
      "result = 2\n",
      "found result = 2\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "q len = 1\n",
      "result = 9\n",
      "found result = 9\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "q len = 1\n",
      "result = 1\n",
      "found result = 1\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "q len = 1\n",
      "result = 7\n",
      "found result = 7\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "q len = 1\n",
      "result = 3\n",
      "found result = 3\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "q len = 1\n",
      "result = 2\n",
      "found result = 2\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "q len = 1\n",
      "result = 9\n",
      "found result = 9\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "q len = 1\n",
      "result = 7\n",
      "found result = 7\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "q len = 1\n",
      "result = 7\n",
      "found result = 7\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "q len = 1\n",
      "result = 6\n",
      "found result = 6\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "q len = 1\n",
      "result = 2\n",
      "found result = 2\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "q len = 1\n",
      "result = 7\n",
      "found result = 7\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "q len = 1\n",
      "result = 8\n",
      "found result = 8\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "q len = 1\n",
      "result = 4\n",
      "found result = 4\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "q len = 1\n",
      "result = 7\n",
      "found result = 7\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "q len = 1\n",
      "result = 3\n",
      "found result = 3\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "q len = 1\n",
      "result = 6\n",
      "found result = 6\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "q len = 1\n",
      "result = 1\n",
      "found result = 1\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "q len = 1\n",
      "result = 3\n",
      "found result = 3\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "q len = 1\n",
      "result = 6\n",
      "found result = 6\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "q len = 1\n",
      "result = 9\n",
      "found result = 9\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "q len = 1\n",
      "result = 3\n",
      "found result = 3\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "q len = 1\n",
      "result = 1\n",
      "found result = 1\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "q len = 1\n",
      "result = 4\n",
      "found result = 4\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "q len = 1\n",
      "result = 1\n",
      "found result = 1\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "q len = 1\n",
      "result = 7\n",
      "found result = 7\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "q len = 1\n",
      "result = 6\n",
      "found result = 6\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "q len = 1\n",
      "result = 9\n",
      "found result = 9\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    q = model.predict( np.array([x_test[i]]) )\n",
    "\n",
    "    print(f\"q len = {len(q)}\")\n",
    "    print(f\"result = {y_test[i]}\" )\n",
    "    print(f\"found result = {np.where(q[0] == numpy.amax(q[0]))[0][0] }\")\n",
    "    ####\n",
    "    ####\n",
    "    ####\n",
    "    ####\n",
    "    ####\n",
    "    ####\n",
    "    ####\n",
    "    ####\n",
    "    ####\n",
    "    ####\n",
    "    ####\n",
    "    ####\n",
    "    ####\n",
    "    ####\n",
    "    ####\n",
    "    ####§§§TODO REVOIR TUTO FLEUR, QUELLE EST LA VALEURE D'UN NEURONE ? puis faire autre tuto nmis tensorflow pour mieux comprendre\n",
    "    ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:26:10) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6a5e76a99c0511a31decff2f843de6b15227c9a58b9fb2d2d9b4eba74a67e12"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
