{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import StringLookup\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.io\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "import random\n",
    "from unidecode import unidecode\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "import uuid\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data\n",
    "Load IAM-Handwritten-Database words, sentences and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam_root_path = './data/IAM-Handwritten-Database/'\n",
    "iam_dataset = [] # format : {\"image_path\": ..., \"laabel\": ...}\n",
    "\n",
    "# Function taht : \n",
    "# opens iam words data descrpition file\n",
    "# If line is not a comment and image is formatted correctly, add the image data\n",
    "# File names in are formated like this :\n",
    "# part1-part2-part3\n",
    "# Files a stored like this on hard drive :\n",
    "# part1/part1-part2/part1-part2-part3.png\n",
    "# filter only well formetted files which size are > 0 bytes\n",
    "# remove every spaces between punctuation, every newline, trailing spaces and vertical bars from label\n",
    "def get_iam_handwritten_db_data(data_type):\n",
    "    dataset = []\n",
    "    with open(os.path.join(iam_root_path, data_type + \".txt\"), 'r') as iam_data_file:\n",
    "        segmentation_result_idx =  1 if data_type == 'words' or data_type == 'line' else 2\n",
    "        lines = [line for line in iam_data_file]\n",
    "        for line in lines:\n",
    "            splitted_line = line.split(' ')\n",
    "            if line[0] != '#' and splitted_line[segmentation_result_idx] != 'err': # if line is not a comment and file is formatted correctly\n",
    "                splitted_image_name = splitted_line[0].split('-')\n",
    "                img_path = os.path.join(\n",
    "                    iam_root_path,\n",
    "                    data_type,\n",
    "                    splitted_image_name[0],\n",
    "                    splitted_image_name[0] + '-' + splitted_image_name[1],\n",
    "                    splitted_line[0] + \".png\"\n",
    "                )\n",
    "                if os.path.exists(img_path) and os.path.getsize(img_path) > 0: #we only keep files that exists and are > 0 bytes\n",
    "                    dataset.append({\n",
    "                        \"image_path\": img_path,\n",
    "                        \"label\": re.sub (r'\\s([?.!\",\\'-;/](?:\\s|$))', r'\\1' , splitted_line[-1].split('\\n')[0].replace('|', ' ').strip()) \n",
    "                    })\n",
    "    return dataset\n",
    "\n",
    "iam_dataset = iam_dataset + get_iam_handwritten_db_data('sentences')\n",
    "iam_dataset = iam_dataset + get_iam_handwritten_db_data('lines')\n",
    "iam_dataset = iam_dataset + get_iam_handwritten_db_data('words')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load IIIT5K-Word V3.0 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "iit_5k_words_root = './data/IIIT5K-Word_V3.0/IIIT5K/'\n",
    "iit_5k_words_dataset = [] # format : {\"image_path\": ..., \"laabel\": ...}\n",
    "\n",
    "# Function taht reads .mat file structure of iit 5k word dataset and returns the formatted data\n",
    "def get_iit_5k_word_data(structure):\n",
    "    dataset = []\n",
    "    for data in structure:\n",
    "        img_path = os.path.join(iit_5k_words_root, data[0][0])\n",
    "        if os.path.exists(img_path) and os.path.getsize(img_path) > 0: #we only keep files that exists and are > 0 bytes\n",
    "            dataset.append({\n",
    "                \"image_path\": img_path,\n",
    "                \"label\": data[1][0]\n",
    "            })\n",
    "    return dataset\n",
    "\n",
    "# Call get_iit_5k_word_data and add train and test data in iit_5k_words_dataset\n",
    "iit_5k_words_dataset = iit_5k_words_dataset + get_iit_5k_word_data(\n",
    "        scipy.io.loadmat(iit_5k_words_root + 'traindata.mat')['traindata'][0]\n",
    ")\n",
    "iit_5k_words_dataset = iit_5k_words_dataset + get_iit_5k_word_data(\n",
    "        scipy.io.loadmat(iit_5k_words_root + 'testdata.mat')['testdata'][0]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load ICDAR 2003 Robust Reading Competitions - Robust Word Recognition dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "icdar_2023_words_root = \"./data/ICDAR 2003 Robust Reading Competitions/Robust Word Recognition/\"\n",
    "icdar_2023_words_dataset = []\n",
    "\n",
    "# Function that read xml file to create formatted dataset\n",
    "def get_icdar_2003_words_data(xml_filepath):\n",
    "    dataset = []\n",
    "    dirname = os.path.dirname(xml_filepath)\n",
    "    image_list = ET.parse(xml_filepath).getroot()\n",
    "\n",
    "    for image in image_list:\n",
    "        img_path = os.path.join(dirname, image.attrib[\"file\"])\n",
    "        if os.path.exists(img_path) and os.path.getsize(img_path): #we only keep files that exists and are > 0 bytes\n",
    "            dataset.append({\n",
    "                \"image_path\": img_path,\n",
    "                \"label\": image.attrib['tag']\n",
    "            })\n",
    "    return dataset\n",
    "    \n",
    "\n",
    "icdar_2023_words_dataset = icdar_2023_words_dataset + get_icdar_2003_words_data(os.path.join(icdar_2023_words_root, \"Sample Set\", \"word.xml\"))\n",
    "icdar_2023_words_dataset = icdar_2023_words_dataset + get_icdar_2003_words_data(os.path.join(icdar_2023_words_root, \"TrialTest Set\", \"word.xml\"))\n",
    "icdar_2023_words_dataset = icdar_2023_words_dataset + get_icdar_2003_words_data(os.path.join(icdar_2023_words_root, \"TrialTrain Set\", \"word.xml\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate EMNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9768dfeb98a54abdbb47e8075eba7938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/683824 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22664\\2964985922.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[0mfrench_dictionnary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'^[a-zA-Z0-9]*$'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrench_dictionnary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;31m# Call function that will create and save handwritten words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m \u001b[0msave_handwritten_words_on_disk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrench_dictionnary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memnist_sorted_by_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;31m#1. Get all characters in a set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22664\\2964985922.py\u001b[0m in \u001b[0;36msave_handwritten_words_on_disk\u001b[1;34m(dictionnary, characters_bank, directory, iterations)\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcreate_word_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcharacters_bank\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'L'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[0mfile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muuid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muuid1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".png\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m             \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m             \u001b[0mdata_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{file_name} {word}\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[0mdata_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, fp, format, **params)\u001b[0m\n\u001b[0;32m   2318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2319\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2320\u001b[1;33m             \u001b[0msave_handler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2321\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2322\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mopen_fp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\PIL\\PngImagePlugin.py\u001b[0m in \u001b[0;36m_save\u001b[1;34m(im, fp, filename, chunk, save_all)\u001b[0m\n\u001b[0;32m   1372\u001b[0m         \u001b[0m_write_multiple_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrawmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1373\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1374\u001b[1;33m         \u001b[0mImageFile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_idat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"zip\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrawmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1376\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\PIL\\ImageFile.py\u001b[0m in \u001b[0;36m_save\u001b[1;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[0;32m    516\u001b[0m                     \u001b[1;31m# compress to Python file-compatible object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m                     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 518\u001b[1;33m                         \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbufsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    519\u001b[0m                         \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from emnist import extract_samples\n",
    "\n",
    "EMNIST_LABEL_TO_CHAR = [\n",
    "    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
    "    'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J','K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T','U', 'V', 'W', 'X', 'Y', 'Z',\n",
    "    'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j','k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't','u', 'v', 'w', 'x', 'y', 'z',\n",
    "]\n",
    "\n",
    "# Functions that returns emnist letters and digits in the following format : {'a': [all 'a' images], 'b': all 'b' images, ...}\n",
    "def get_emnist_sorted_by_label():\n",
    "    dataset = {}\n",
    "    emnist_train_images, emnist_train_labels = extract_samples('byclass', 'train')\n",
    "    emnist_test_images, emnist_test_labels = extract_samples('byclass', 'test')\n",
    "    images, labels = np.concatenate((emnist_train_images, emnist_test_images)), np.concatenate((emnist_train_labels, emnist_test_labels))\n",
    "    labels_set = sorted(list(set(label for label in labels)))\n",
    "    \n",
    "    for unique_label_idx in labels_set:        \n",
    "        matching_indices = [i for i in range(len(labels)) if labels[i] == unique_label_idx]\n",
    "        dataset[EMNIST_LABEL_TO_CHAR[unique_label_idx]] = list(\n",
    "            map(\n",
    "                lambda index: images[index] ,\n",
    "                matching_indices\n",
    "            )\n",
    "        )\n",
    "    return dataset\n",
    "\n",
    "# Function that adds up EMNIST images array to create given word\n",
    "def create_word_matrix(word, characters_bank):\n",
    "    word_matrix = np.zeros((28, len(word) * 28))\n",
    "    for char_idx in range(len(word)):\n",
    "        random_letter_matrix = random.choice(characters_bank[word[char_idx]]) if word[char_idx] != ' ' else np.zeros((28, 28))\n",
    "        word_matrix[0:28, (char_idx) * 28:(char_idx + 1) * 28] = random_letter_matrix\n",
    "    return word_matrix\n",
    "\n",
    "# Function that loop over all dictionnary words, create and save handwritten words\n",
    "def save_handwritten_words_on_disk(dictionnary, characters_bank, directory=\"./data/EMNIST-Handwritten-French-Words/\", iterations=1):\n",
    "    # Create directory if does not exists\n",
    "    if os.path.exists(directory) == False: os.makedirs(directory)\n",
    "    # Open and create data file if does not exists\n",
    "    data_file = open(os.path.join(directory, \"data.txt\"), \"a+\")\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        for word in tqdm(dictionnary):\n",
    "            im = Image.fromarray(create_word_matrix(word, characters_bank)).convert('L')\n",
    "            file_name = str(uuid.uuid1()) + \".png\"\n",
    "            im.save(os.path.join(directory, file_name))\n",
    "            data_file.write(f\"{file_name} {word}\\n\")\n",
    "    data_file.close()\n",
    "\n",
    "# Create formatted EMNIST label dataset\n",
    "emnist_sorted_by_label = get_emnist_sorted_by_label()\n",
    "#Open french dictionnary file\n",
    "french_dictionnary = open('./data/dela-fr-public.txt').readlines()\n",
    "# Transform words so they have no accents\n",
    "french_dictionnary = list(map(lambda line:  unidecode(line.split(',')[0]), french_dictionnary))\n",
    "# Filter to remove words with special characters\n",
    "french_dictionnary = list(filter(lambda word: bool(re.match('^[a-zA-Z0-9]*$', word)), french_dictionnary))\n",
    "# Call function that will create and save handwritten words\n",
    "save_handwritten_words_on_disk(french_dictionnary, emnist_sorted_by_label)\n",
    "\n",
    "#1. Get all characters in a set\n",
    "#2. Get a dict with this format : {'a': [all 'a' images], 'b': all 'b' images, ...}\n",
    "#3. Find a French dictionnary adn for each words\n",
    "#4. Create x time this word by adding matrices and creating new ones\n",
    "#5. Do the same but with sentences\n",
    "#6. Save on hard drive since tensorflow will need to load on hard drive\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96454\n",
      "5000\n",
      "1666\n"
     ]
    }
   ],
   "source": [
    "dataset = iam_words_dataset + iit_5k_words_dataset + icdar_2023_words_dataset\n",
    "print(dataset[0])\n",
    "np.random.shuffle(dataset)\n",
    "# print(dataset[:1000])\n",
    "# len(iam_words_dataset) = 96454\n",
    "# len(iit_5k_words_dataset) = 5000\n",
    "# len(icdar_2023_words_dataset) = 1666\n",
    "#\n",
    "\n",
    "# subject_labels\n",
    "# eoc_labels\n",
    "# alphabet\n",
    "# strokes\n",
    "# eow_labels\n",
    "# char_labels\n",
    "# word_labels\n",
    "# max\n",
    "# min\n",
    "# soc_labels\n",
    "# mean\n",
    "# texts\n",
    "# std\n",
    "# preprocessing\n",
    "# sow_labels\n",
    "\n",
    "\n",
    "#### !!!! CHECK SI EN RESIZANT LES IMAGES EMNIST AVE TF ON A UN TRUC ENCORE LISIBLE\n",
    "\n",
    "## split in 98:1:1\n",
    "## shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20372\\531738288.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmax_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;31m# We split the line by spaces, take the last value (which is the actual word) and strip this\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_labels' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
